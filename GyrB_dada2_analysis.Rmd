---
title: "dada2_gyrB_analysis"
author: "Robert Nichols"
date: "3/17/2023"
output: html_document
---
# Dada2 analysis of PacBio and MiSeq GyrB sequencing data

This script goes through the DADA2 analysis pipeline (tutorial found at https://benjjneb.github.io/dada2/tutorial.html) for the *gyrB* sequencing data obtained from both the PacBio and MiSeq sequencing machines. For the purposes of this script we assume that the dada2 program has already been installed. The dada2 version used for this script is 1.28.0

##### Set working directory in the session setup. Change the root.dir to where the downloaded test data is located 
```{r setup}
require("knitr")
opts_knit$set(root.dir ="~/Desktop/GyrB_Test_Data/")
```

##### Load libraries:
```{r}
library(dada2)
```

##### Input and output file paths:
```{r}
# These Paths originate with the downloaded folder, make sure to change your working directory to the downloaded test folder.


#For Pacbio data
path_pac_Bac <- "PacBio/Bac/"
path_pac_Lac <- "PacBio/Lac/"
path_pac_Bif <- "PacBio/Bif/"
path_pac_16S <- "PacBio/16S/"

#For MiSeq data
path_mis_Bac <- "MiSeq/Bac/"
path_mis_Lac <- "MiSeq/Lac/"
path_mis_Bif <- "MiSeq/Bif/"
path_mis_16S <- "MiSeq/16S/"

#For Pseudo MiSeq Data
path_pseudo_Bac <- "Pseudo_MiSeq/Bac/"
path_pseudo_Bif <- "Pseudo_MiSeq/Bif/"
path_pseudo_Lac <- "Pseudo_MiSeq/Lac/"
```

## Analyze Bacteroidaceae gyrB PacBio sequences

##### Set primer sequences and read in sequence data 
```{r}
set.seed(100)

# These are the primers created with phylotags, without the PacBio overhangs
F_Bac <- "CCYGCGATGTAYATYGGTGAC"
R_Bac <- "ACYTGYTTCAGCATACGGTTT"

# Input the fasta sequences from the input directory  
fns_bac <- list.files(path_pac_Bac, pattern="fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fns_bac

# [1] "PacBio/Bac//Bac_GM_pac.sub.fastq" "PacBio/Bac//Bac_H1_pac.sub.fastq"
# [3] "PacBio/Bac//Bac_H2_pac.sub.fastq" "PacBio/Bac//Bac_H3_pac.sub.fastq"
# [5] "PacBio/Bac//Bac_H4_pac.sub.fastq" "PacBio/Bac//Bac_H5_pac.sub.fastq"
# [7] "PacBio/Bac//Bac_M1_pac.sub.fastq" "PacBio/Bac//Bac_M2_pac.sub.fastq"

```


##### Remove primers, filter out low quality reads and filter out reads that are the wrong size
```{r}
# Create directory for filtered reads
nops2_bac <- file.path(path_pac_Bac, "noprimers", basename(fns_bac))

# Remove primers from the PacBio sequences
for(i in seq_along(fns_bac)){
  fn_bac <- fns_bac[[i]]; nop_bac <- nops2_bac[[i]]
  dada2:::removePrimers(fn_bac, nop_bac, primer.fwd=F_Bac, primer.rev=dada2:::rc(R_Bac), orient=TRUE)}

# Filter and trim reads with a minimum quality score of 3, minimum length of 1000bp, maximum length of 2000bp, zero ambiguous bases (Ns), don't remove phiX sequences, and a maximum expected error rate (maxEE) of 3.
filts2_bac <- file.path(path_pac_Bac, "noprimers", "filtered", basename(fns_bac))
track2_bac <- filterAndTrim(nops2_bac, filts2_bac, minQ=3, minLen=1000, maxLen=2000, maxN=0, rm.phix = FALSE, maxEE=3)

# print statistics for how many reads were filtered out
track2_bac

#                      reads.in reads.out
# Bac_GM_pac.sub.fastq     5467      3790
# Bac_H1_pac.sub.fastq     7415      4909
# Bac_H2_pac.sub.fastq     5701      3891
# Bac_H3_pac.sub.fastq     9402      6400
# Bac_H4_pac.sub.fastq     6488      5306
# Bac_H5_pac.sub.fastq     2673      2194
# Bac_M1_pac.sub.fastq     6879      4723
# Bac_M2_pac.sub.fastq     6508      4541
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences

```{r}
# Dereplicate filtered files:
drep2_bac <- derepFastq(filts2_bac, verbose=TRUE)

# Learn error rates specific for PacBio sequencing:
err2_bac <- learnErrors(drep2_bac, errorEstimationFunction = PacBioErrfun, BAND_SIZE=32, multithread = TRUE)

# Call ASVs for sequences
dd2_bac <- dada(drep2_bac, err=err2_bac, multithread = TRUE)

# Make a sequence table across all ASVs
st2_bac <- makeSequenceTable(dd2_bac)

# Check for the number if chimeric sequences 
bim2_bac <- isBimeraDenovo(st2_bac, minFoldParentOverAbundance = 3.5, multithread = TRUE)
table(bim2_bac)

# FALSE  TRUE 
#    19    39 

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_bac <- removeBimeraDenovo(st2_bac, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_bac)/sum(st2_bac)
# [1] 0.9761603
```

##### Relabel samples and and assign taxonomy

```{r}
# Relabel chimera filtered samples with appropriate sample names. This command is path dependent so it needs to be modified based on where the test directory is 
sample.names_bac <- sapply(strsplit(fns_bac,"/"), function(x) paste(x[8]))
row.names(seqtab.nochim_bac) <- sample.names_bac

# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file:
bac_gryB_tax <- assignTaxonomy(seqtab.nochim_bac, "gyr.refseq.fa.s.fa")

### reading FASTA file gyr.refseq.fa.s.fa: ignored 6472 invalid one-letter sequence codes 
```

##### Write out ASV table and taxonomy assignments

```{r}
# Write out ASV table
write.csv(seqtab.nochim_bac, file="Output/bac_gyrB_pac_seq.csv") 

# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast. 
write.csv(bac_gryB_tax, file="Output/bac_gyrB_pac_tax.csv") 
```

## Analyze Lachnospiraceae gyrB PacBio sequences

##### Set primer sequences and read in sequence data 
```{r}
set.seed(100)

# These are the primers created with phylotags, without the PacBio overhangs
F_Lach <-"SAGRGGWCTBCATCATYTRGT"
R_Lach <-"TCMGGATCCATDGTBGTCTCC"

# Input the fasta sequences from the input directory  
fns_lac <- list.files(path_pac_Lac, pattern="fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fns_lac
```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size
```{r}
# Create directory for filtered reads
nops2_lac <- file.path(path_pac_Lac, "noprimers", basename(fns_lac))

# Remove primers from the PacBio sequences 
for(i in seq_along(fns_lac)){
  fn_lac <- fns_lac[[i]]; nop_lac <- nops2_lac[[i]]
  dada2:::removePrimers(fn_lac, nop_lac, primer.fwd=F_Lach, primer.rev=dada2:::rc(R_Lach), orient=TRUE)
}

# Filter and trim reads with a minimum quality score of 3, minimum length of 1000bp, maximum length of 2000bp, zero ambiguous bases (Ns), don't remove phiX sequences, and a maximum expected error rate (maxEE) of 3.

filts2_lac <- file.path(path_pac_Lac, "noprimers", "filtered", basename(fns_lac))

track2_lac <- filterAndTrim(nops2_lac, filts2_lac, minQ=3, minLen=1000, maxLen=2000, maxN=0, rm.phix = FALSE, maxEE=3)

# print statistics for how many reads were filtered out
track2_lac

#                      reads.in reads.out
# Lac_GM_pac.sub.fastq     9866      6466
# Lac_H1_pac.sub.fastq    15703     11068
# Lac_H2_pac.sub.fastq    14004      9812
# Lac_H3_pac.sub.fastq     9581      6723
# Lac_H4_pac.sub.fastq    20077     13744
# Lac_H5_pac.sub.fastq    19904     13967
# Lac_M1_pac.sub.fastq    16464     10689
# Lac_M2_pac.sub.fastq    11154      7450
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences
```{r}
# Dereplicate filtered files
drep2_lac <- derepFastq(filts2_lac, verbose=TRUE)

# Learn error rates specific for PacBio sequencing
err2_lac <- learnErrors(drep2_lac, errorEstimationFunction = PacBioErrfun, BAND_SIZE=32, multithread = TRUE)

# Call ASVs for sequences
dd2_lac <- dada(drep2_lac, err=err2_lac, multithread = TRUE)

# Make a sequence table across all ASVs
st2_lac <- makeSequenceTable(dd2_lac) 

# Check for the number if chimeric sequences 
bim2_lac <- isBimeraDenovo(st2_lac, minFoldParentOverAbundance = 3.5, multithread = TRUE)
table(bim2_bac)

# FALSE  TRUE 
#    19    39 

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_lac <- removeBimeraDenovo(st2_lac, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_lac)/sum(st2_lac)

# [1] 0.9533761
```

##### Relabel samples and and assign taxonomy
```{r}
# Relabel chimera filtered samples with appropriate sample names. This command is path dependent so it needs to be modified based on where the test directory is 

sample.names_lac <- sapply(strsplit(fns_lac,"/"), function(x) paste(x[8]))
row.names(seqtab.nochim_lac) <- sample.names_lac

# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file:
Lach_gryB_tax <- assignTaxonomy(seqtab.nochim_lac, "gyr.refseq.fa.s.fa")


```

##### Write out ASV table and taxonomy assignments

```{r}
# Write out ASV table
write.csv(seqtab.nochim_lac, file="Output/lach_gyrB_pac_seq.csv")

# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast.
write.csv(Lach_gryB_tax, file="Output/lach_gyrB_pac_tax.csv")

```


## Analyze Bifidobacteriaceae gyrB PacBio sequences

##### Set primer sequences and read in sequence data 

```{r}
set.seed(100)

# These are the primers created with phylotags, without the PacBio overhangs

F_Bif <- "ATCGARGTSACGATTCTGCCG"
R_Bif <- "GGATCCATGGTGGTYTCCCAC"

# Input the fasta sequences from the input directory  
fns_bif <- list.files(path_pac_Bif, pattern="fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fns_bif
```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size
```{r}
# Create directory for filtered reads
nops2_bif <- file.path(path_pac_Bif, "noprimers", basename(fns_bif))

# Remove primers from the PacBio sequences 
for(i in seq_along(fns_bif)){
  fn_bif <- fns_bif[[i]]; nop_bif <- nops2_bif[[i]]
  dada2:::removePrimers(fn_bif, nop_bif, primer.fwd=F_Bif, primer.rev=dada2:::rc(R_Bif), orient=TRUE)
}

# Filter and trim reads with a minimum quality score of 3, minimum length of 1000bp, maximum length of 2000bp, zero ambiguous bases (Ns), don't remove phiX sequences, and a maximum expected error rate (maxEE) of 3.
filts2_bif <- file.path(path_pac_Bif, "noprimers", "filtered", basename(fns_bif))


track2_bif <- filterAndTrim(nops2_bif, filts2_bif, minQ=3, minLen=1000, maxLen=2000, maxN=0, rm.phix = FALSE, maxEE=3)

# print statistics for how many reads were filtered out
track2_bif

#                     reads.in reads.out
# Bif_GM_pac.sub.fastq    16057      5454
# Bif_H1_pac.sub.fastq     6325      4321
# Bif_H2_pac.sub.fastq    11043      4566
# Bif_H4_pac.sub.fastq     8868      6020
# Bif_H5_pac.sub.fastq     3977      2797
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences

```{r}
# Dereplicate filtered files:
drep2_bif <- derepFastq(filts2_bif, verbose=TRUE)

# Learn error rates specific for PacBio sequencing:
err2_bif <- learnErrors(drep2_bif, errorEstimationFunction = PacBioErrfun, BAND_SIZE=32, multithread = TRUE)

# Call ASVs for sequences
dd2_bif <- dada(drep2_bif, err=err2_bif, multithread = TRUE)

# Make a sequence table across all ASVs
st2_bif <- makeSequenceTable(dd2_bif)

# Check for the number if chimeric sequences 
bim2_bif <- isBimeraDenovo(st2_bif, minFoldParentOverAbundance = 3.5, multithread = TRUE)
table(bim2_bac)

# FALSE  TRUE 
#    19    39 

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_bif <- removeBimeraDenovo(st2_bif, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_bif)/sum(st2_bif)

# [1] 0.9603003
```

##### Relabel samples and and assign taxonomy
```{r}
# Relabel chimera filtered samples with appropriate sample names. This command is path dependent so it needs to be modified based on where the test directory is 
sample.names_bif <- sapply(strsplit(fns_bif,"/"), function(x) paste(x[8]))
row.names(seqtab.nochim_bif) <- sample.names_bif

# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file
Bif_gryB_tax <- assignTaxonomy(seqtab.nochim_bif, "gyr.refseq.fa.s.fa")


```

##### Write out ASV table and taxonomy assignments
```{r}
# Write out ASV table
write.csv(seqtab.nochim_bif, file="Output/bif_gyrB_pac_seq.csv")

# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast.
write.csv(Bif_gryB_tax, file="Output/bif_gyrB_pac_tax.csv")
```

## Analyze 16S PacBio sequences

##### Set primer sequences and read in sequence data 
```{r}
set.seed(100)
#These are the primers created with phylotags, without the PacBio add-on
F27 <- "AGRGTTYGATYMTGGCTCAG"
R1492 <- "RGYTACCTTGTTACGACTT"

# Input the fasta sequences from the input directory  
fns_16S_P <- list.files(path_pac_16S, pattern="fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fns_16S_P
```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size
```{r}
# Create directory for filtered reads
nops2_16S_P <- file.path(path_pac_16S, "noprimers", basename(fns_16S_P))

# Remove primers from the PacBio sequences 
for(i in seq_along(fns_16S_P)){
  fn_16S_P <- fns_16S_P[[i]]; nop_16S_P <- nops2_16S_P[[i]]
  dada2:::removePrimers(fn_16S_P, nop_16S_P, primer.fwd=F27, primer.rev=dada2:::rc(R1492), orient=TRUE)}

# Filter and trim reads with a minimum quality score of 3, minimum length of 1000bp, maximum length of 2000bp, zero ambiguous bases (Ns), don't remove phiX sequences, and a maximum expected error rate (maxEE) of 3.
filts2_16S_P <- file.path(path_pac_16S, "noprimers", "filtered", basename(fns_16S_P))

track2_16S_P <- filterAndTrim(nops2_16S_P, filts2_16S_P, minQ=3, minLen=1000, maxLen=2000, maxN=0, rm.phix = FALSE, maxEE=3)

# print statistics for how many reads were filtered out
track2_16S_P
#                      reads.in reads.out
# 16S_GM_pac.sub.fastq    14390      9364
# 16S_H1_pac.sub.fastq     6497      4257
# 16S_H2_pac.sub.fastq     6014      3890
# 16S_H3_pac.sub.fastq     5913      3966
# 16S_H4_pac.sub.fastq    10358      6515
# 16S_H5_pac.sub.fastq     7962      5336
# 16S_M1_pac.sub.fastq     7114      4594
# 16S_M2_pac.sub.fastq     8408      5483
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences
```{r}
# Dereplicate filtered files:
drep2_16S_P <- derepFastq(filts2_16S_P, verbose=TRUE)

# Learn error rates specific for PacBio sequencing:
err2_16S_P <- learnErrors(drep2_16S_P, errorEstimationFunction = PacBioErrfun, BAND_SIZE=32, multithread = TRUE)

# Call ASVs for sequences
dd2_16S_P <- dada(drep2_16S_P, err=err2_16S_P, multithread = TRUE)

# Make a sequence table across all ASVs
st2_16S_P <- makeSequenceTable(dd2_16S_P) ; dim(st2_16S_P)
# 8 707

# Check for the number if chimeric sequences 
bim2_16S_P <- isBimeraDenovo(st2_16S_P, minFoldParentOverAbundance = 3.5, multithread = TRUE)
table(bim2_16S_P)
# FALSE  TRUE 
#   544   163 


# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_16S_P <- removeBimeraDenovo(st2_16S_P, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_16S_P)/sum(st2_16S_P)
# 0.8535032
```

##### Relabel samples and and assign taxonomy
```{r}
# Relabel chimera filtered samples with appropriate sample names. This command is path dependent so it needs to be modified based on where the test directory is 

sample.names_16S_P <- sapply(strsplit(fns_16S_P,"/"), function(x) paste(x[8]))
row.names(seqtab.nochim_16S_P)<-sample.names_16S_P

# Assign taxonomy using the silva_nr99_v138.1_train_set.fa.gz file that can be downloaded from https://zenodo.org/record/4587955 (and is mentioned in the dada2 tutorial referenced above)
taxa_16S_P <- assignTaxonomy(seqtab.nochim_16S_P, "silva_nr99_v138.1_train_set.fa.gz")

#Add species info with the silva_species_assignment_v138.1.fa.gz
taxa_16S_P_species <- addSpecies(taxa_16S_P, "silva_species_assignment_v138.1.fa.gz")
```

##### Write out ASV table and taxonomy assignments
```{r}
# Write out ASV table
write.csv(seqtab.nochim_16S_P, file="Output/16S_gyrB_pac_seq.csv")

# Write out taxonomy file.  
write.csv(taxa_16S_P_species, file="Output/16S_gyrB_pac_tax.csv")
```


## Analyze Bacteroidaceae gyrB MiSeq  sequences

##### Set primer sequences and read in sequence data 

```{r}
set.seed(100)

# Input the fasta sequences from the input directory. Its important to import both the forward and reverse reads. Note the pattern option may be different for you reads. Make sure to change this based on what text is after 'R1' and "R2" for your forward and reverse reads.   

fnFs_Bac_N <- list.files(path_mis_Bac, pattern="R1.sub.fastq", full.names = TRUE)
fnRs_Bac_N <- list.files(path_mis_Bac, pattern="R2.sub.fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fnFs_Bac_N
fnRs_Bac_N 

# Set sample names. Note that this command takes everything before the first underscore for each sample and uses that text as the sample name. So for example the title for sample 1 is "Bac-H1_R1.sub.fastq" and this command will name sample one as Bac-H1. 
sample.names_bac_N <- sapply(strsplit(basename(fnFs_Bac_N), "_"), `[`, 1)
```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size
```{r}
# Create directory for filtered forward and reverse reads 
filtFs_bac_N <- file.path(path_mis_Bac, "filtered", paste0(sample.names_bac_N, "_F_filt.fastq.gz"))
filtRs_bac_N <- file.path(path_mis_Bac, "filtered", paste0(sample.names_bac_N, "_R_filt.fastq.gz"))

# rename the samples 
names(filtFs_bac_N) <- sample.names_bac_N
names(filtRs_bac_N) <- sample.names_bac_N

# Filter and trim reads with a minimum quality score of 2 (truncQ), maximum length of 2000bp, zero ambiguous bases (Ns), remove phiX sequences, and a maximum expected error rate (maxEE) of 2. Forward reads were cut after 250 bp and the reverse reads were trimmed after 230 bp. Primers are removed with the trimLeft command, values indicate primer bp sizes to be removed from the forward and reverse reads. 
out_bac_N <- filterAndTrim(fnFs_Bac_N, filtFs_bac_N, fnRs_Bac_N, filtRs_bac_N, truncLen=c(250,230), trimLeft = c(21,21), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)

#See tracked read statistics 
out_bac_N 
#                     reads.in reads.out
# Bac-GM_R1.sub.fastq    10865      9549
# Bac-H1_R1.sub.fastq    14452     13013
# Bac-H2_R1.sub.fastq    11367      9907
# Bac-H3_R1.sub.fastq     8849      7414
# Bac-H4_R1.sub.fastq     6564      6056
# Bac-H5_R1.sub.fastq    13665     12790
# Bac-M1_R1.sub.fastq    12366     11358
# Bac-M2_R1.sub.fastq    13637     12522
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences

```{r}

# Learn error rates specific for MiSeq sequencing
errF_bac_N <- learnErrors(filtFs_bac_N, multithread=TRUE)
errR_bac_N <- learnErrors(filtRs_bac_N, multithread=TRUE)

# Call ASVs for sequences
dadaFs_bac_N <- dada(filtFs_bac_N, err=errF_bac_N, multithread=TRUE)
dadaRs_bac_N <- dada(filtRs_bac_N, err=errR_bac_N, multithread=TRUE)

#make contigs between the forward and reverse reads. Notice the 'justConcatenate=TRUE' flag. This is included to tell dada2 to just concatenate the reads even if there is not an overlap, which is the case for these reads.
mergers_bac_N <- mergePairs(dadaFs_bac_N, filtFs_bac_N, dadaRs_bac_N, filtRs_bac_N, verbose=TRUE, justConcatenate = TRUE)

# Make a sequence table across all ASVs
seqtab_bac_N <- makeSequenceTable(mergers_bac_N)


# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_bac_N <- removeBimeraDenovo(seqtab_bac_N, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_bac_N)/sum(seqtab_bac_N)


#Track sequences throughout the analysis to look for large read loss after certain steps
getN <- function(x) sum(getUniques(x))


track_bac_N <- cbind(out_bac_N, sapply(dadaFs_bac_N, getN), sapply(dadaRs_bac_N, getN), sapply(mergers_bac_N, getN), rowSums(seqtab.nochim_bac_N))
colnames(track_bac_N) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track_bac_N) <- sample.names_bac_N

track_bac_N

#        input filtered denoisedF denoisedR merged nonchim
# Bac-GM 10865     9549      9537      9534   9533    9533
# Bac-H1 14452    13013     13001     12995  12991   12991
# Bac-H2 11367     9907      9897      9888   9887    9887
# Bac-H3  8849     7414      7397      7384   7372    6836
# Bac-H4  6564     6056      6015      6021   5990    5636
# Bac-H5 13665    12790     12772     12772  12765   12763
# Bac-M1 12366    11358     11349     11345  11343   11341
# Bac-M2 13637    12522     12508     12504  12501   12497
```

##### Assign taxonomy and export files 

```{r}
# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file

Bac_Moeller_to_pac_tax_N <- assignTaxonomy(seqtab.nochim_bac_N, "gyr.refseq.fa.s.fa")



# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast.
write.csv(Bac_Moeller_to_pac_tax_N, file="Output/Bac_Moeller_tax_N.csv")

# Write out ASV table
write.csv(seqtab.nochim_bac_N, file="Output/Bac_Moeller_seq.csv")
```

## Analyze Bifidobacteriaceae. gyrB MiSeq  sequences

##### Set primer sequences and read in sequence data 

```{r}
# Note only forward Bifidobacteriaceae reads were used as per the orginal analysis. 
set.seed(100)

# Input the fasta sequences from the input directory. Its important to import both the forward and reverse reads. Note the pattern option may be different for you reads. Make sure to change this based on what text is after 'R1' and "R2" for your forward and reverse reads.
fnFs_bif_F <- list.files(path_mis_Bif, pattern="R1.sub.fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones
fnFs_bif_F 

# Set sample names. Note that this command takes everything before the first underscore for each sample and uses that text as the sample name.
sample.names_bif_F <- sapply(strsplit(basename(fnFs_bif_F), "_"), `[`, 1)



```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size

```{r}
# Create directory for filtered forward reads 
filtFs_bif_f <- file.path(path_mis_Bif, "filtered", paste0(sample.names_bif_F, "_F_filt.fastq.gz"))

# rename the samples 
names(filtFs_bif_f) <- sample.names_bif_F

# Filter and trim reads with a minimum quality score of 2 (truncQ), maximum length of 2000bp, zero ambiguous bases (Ns), remove phiX sequences, and a maximum expected error rate (maxEE) of 2. Forward reads were cut after 250 bp. Primers are removed with the trimLeft command, values indicate primer bp sizes to be removed from the forward reads. 
out_bif_f <- filterAndTrim(fnFs_bif_F, filtFs_bif_f, truncLen=c(250), trimLeft = c(21), maxN=0, maxEE=c(2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)

#See tracked read statistics 
out_bif_f

#                     reads.in reads.out
# Bif-GM_R1.sub.fastq    41490     14607
# Bif-H1_R1.sub.fastq    56329     18859
# Bif-H2_R1.sub.fastq    46633     13863
# Bif-H3_R1.sub.fastq    34050     13016
# Bif-H4_R1.sub.fastq    21311      8237
# Bif-H5_R1.sub.fastq    44519     18155
# Bif-M1_R1.sub.fastq    58756     12085
# Bif-M2_R1.sub.fastq    51642      9672
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences

```{r}
# Learn error rates specific for MiSeq sequencing
errF_bif_f <- learnErrors(filtFs_bif_f, multithread=TRUE)

# Call ASVs for sequences
dadaFs_bif_f <- dada(filtFs_bif_f, err=errF_bif_f, multithread=TRUE)

# Make a sequence table across all ASVs
seqtab_bif_f <- makeSequenceTable(dadaFs_bif_f)

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_bif_f <- removeBimeraDenovo(seqtab_bif_f, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_bif_f)/sum(seqtab_bif_f)

#Track sequences throughout the analysis to look for large read loss after certain steps

track_bif_f<- cbind(out_bif_f, sapply(dadaFs_bif_f, getN), rowSums(seqtab.nochim_bif_f))
colnames(track_bif_f) <- c("input", "filtered", "denoisedF", "nonchim")
rownames(track_bif_f) <- sample.names_bif_F
track_bif_f

#        input filtered denoisedF nonchim
# Bif-GM 41490    14607     14375   14375
# Bif-H1 56329    18859     18658   18527
# Bif-H2 46633    13863     13560   13085
# Bif-H3 34050    13016     12774   11666
# Bif-H4 21311     8237      8088    7836
# Bif-H5 44519    18155     17939   17572
# Bif-M1 58756    12085     11810   11808
# Bif-M2 51642     9672      9348    9348
```

##### Assign taxonomy and export files 

```{r}
# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file

Bif_moeller_to_pac_txt <- assignTaxonomy(seqtab.nochim_bif_f, "gyr.refseq.fa.s.fa")


# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast.
write.csv(Bif_moeller_to_pac_txt, file="Output/Bif_Moeller_tax_N.csv")

# Write out ASV table
write.csv(seqtab.nochim_bif_f, file="Output/Bif_Moeller_seq.csv")
```

## Analyze Lachnospiraceae gyrB MiSeq  sequences

##### Set primer sequences and read in sequence data
```{r}
set.seed(100)

# Input the fasta sequences from the input directory. Its important to import both the forward and reverse reads. Note the pattern option may be different for you reads. Make sure to change this based on what text is after 'R1' and "R2" for your forward and reverse reads.  

fnFs_Lac_M <- list.files(path_mis_Lac, pattern="R1.sub.fastq", full.names = TRUE)
fnRs_Lac_M <- list.files(path_mis_Lac, pattern="R2.sub.fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fnFs_Lac_M 
fnRs_Lac_M

# Set sample names

sample.names_Lac_M <- sapply(strsplit(basename(fnFs_Lac_M), "_"), `[`, 1)

```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size

```{r}
# Create directory for filtered forward and reverse reads 
filtFs_Lac_M <- file.path(path_mis_Lac, "filtered", paste0(sample.names_Lac_M, "_F_filt.fastq.gz"))
filtRs_Lac_M <- file.path(path_mis_Lac, "filtered", paste0(sample.names_Lac_M, "_R_filt.fastq.gz"))

# rename the samples
names(filtFs_Lac_M) <- sample.names_Lac_M
names(filtRs_Lac_M) <- sample.names_Lac_M

# Filter and trim reads with a minimum quality score of 2 (truncQ), maximum length of 2000bp, zero ambiguous bases (Ns), remove phiX sequences, and a maximum expected error rate (maxEE) of 2. Forward reads were cut after 250 bp and the reverse reads were trimmed after 230 bp. Primers are removed with the trimLeft command, values indicate primer bp sizes to be removed from the forward and reverse reads. 
out_Lac_M <- filterAndTrim(fnFs_Lac_M, filtFs_Lac_M, fnRs_Lac_M, filtRs_Lac_M, truncLen=c(250,230), trimLeft = c(23,21), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)

#See tracked read statistics 
out_Lac_M

#                     reads.in reads.out
# Lac-GM_R1.sub.fastq    23033     21631
# Lac-H1_R1.sub.fastq    15202     13369
# Lac-H2_R1.sub.fastq    18549     17457
# Lac-H3_R1.sub.fastq    15243     12638
# Lac-H4_R1.sub.fastq    14039     13429
# Lac-H5_R1.sub.fastq    14556     14025
# Lac-M1_R1.sub.fastq    17127     16354
# Lac-M2_R1.sub.fastq    17037     16402
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences

```{r}
# Learn error rates specific for MiSeq sequencing
errF_Lac_M <- learnErrors(filtFs_Lac_M, multithread=TRUE)
errR_Lac_M <- learnErrors(filtRs_Lac_M, multithread=TRUE)

# Call ASVs for sequences
dadaFs_Lac_M <- dada(filtFs_Lac_M, err=errF_Lac_M, multithread=TRUE)
dadaRs_Lac_M <- dada(filtRs_Lac_M, err=errR_Lac_M, multithread=TRUE)

#make contigs between the forward and reverse reads. Notice the 'justConcatenate=TRUE' flag. This is included to tell dada2 to just concatenate the reads even if there is not an overlap, which is the case for these reads.
mergers_Lac_M <- mergePairs(dadaFs_Lac_M, filtFs_Lac_M, dadaRs_Lac_M, filtRs_Lac_M, verbose=TRUE, justConcatenate = TRUE)

# Make a sequence table across all ASVs
seqtab_Lac_M <- makeSequenceTable(mergers_Lac_M)

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_Lac_M <- removeBimeraDenovo(seqtab_Lac_M, method="consensus", multithread=TRUE, verbose=TRUE)

#Track sequences throughout the analysis to look for large read loss after certain steps
track_Lac_M <- cbind(out_Lac_M, sapply(dadaFs_Lac_M, getN), sapply(dadaRs_Lac_M, getN), sapply(mergers_Lac_M, getN), rowSums(seqtab.nochim_Lac_M))
colnames(track_Lac_M) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track_Lac_M) <- sample.names_Lac_M
track_Lac_M

#        input filtered denoisedF denoisedR merged nonchim
# Lac-GM 23033    21631     21617     21605  21604   21604
# Lac-H1 15202    13369     13297     13317  13254   12868
# Lac-H2 18549    17457     17386     17374  17319   16594
# Lac-H3 15243    12638     12604     12583  12565   11185
# Lac-H4 14039    13429     13381     13371  13338   11961
# Lac-H5 14556    14025     13966     13967  13919   12842
# Lac-M1 17127    16354     16295     16291  16249   15621
# Lac-M2 17037    16402     16327     16331  16279   15947
```

##### Assign taxonomy and export files 
```{r}
# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file
Lac_Moeller_to_pac_tax_M <- assignTaxonomy(seqtab.nochim_Lac_M, "gyr.refseq.fa.s.fa")

# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast.
write.csv(Lac_Moeller_to_pac_tax_M, file="Output/Lac_Moeller_tax_M.csv")

# Write out ASV table
write.csv(seqtab.nochim_Lac_M , file="Output/Lac_moeller_seq.csv")

```

## Analyze 16S MiSeq  sequences

##### Set primer sequences and read in sequence data 

```{r}
set.seed(100)
# Input the fasta sequences from the input directory. Its important to import both the forward and reverse reads. Note the pattern option may be different for you reads. Make sure to change this based on what text is after 'R1' and "R2" for your forward and reverse reads.  
fnFs_16S_M <- list.files(path_mis_16S, pattern="R1.sub.fastq", full.names = TRUE)
fnRs_16S_M <- list.files(path_mis_16S, pattern="R2.sub.fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fnFs_16S_M
fnRs_16S_M

# Set sample names 
sample.names_16S_M <- sapply(strsplit(basename(fnFs_16S_M), "_"), `[`, 1)
```


##### Remove primers, filter out low quality reads and filter out reads that are the wrong size
```{r}
# Create directory for filtered forward and reverse reads 
filtFs_16S_M <- file.path(path_mis_16S, "filtered", paste0(sample.names_16S_M, "_F_filt.fastq.gz"))
filtRs_16S_M <- file.path(path_mis_16S, "filtered", paste0(sample.names_16S_M, "_R_filt.fastq.gz"))

# rename the samples 
names(filtFs_16S_M) <- sample.names_16S_M
names(filtRs_16S_M) <- sample.names_16S_M

# Filter and trim reads with a minimum quality score of 2 (truncQ), maximum length of 2000bp, zero ambiguous bases (Ns), remove phiX sequences, and a maximum expected error rate (maxEE) of 2. Forward reads were cut after 250 bp and the reverse reads were trimmed after 230 bp. Primers are removed with the trimLeft command, values indicate primer bp sizes to be removed from the forward and reverse reads. 
out_16S_M <- filterAndTrim(fnFs_16S_M, filtFs_16S_M, fnRs_16S_M, filtRs_16S_M, truncLen=c(250,230), trimLeft = c(20,21), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)

#See tracked read statistics 
out_16S_M

#                     reads.in reads.out
# 16S-GM_R1.sub.fastq    32445     28978
# 16S-H1_R1.sub.fastq    31653     28178
# 16S-H2_R1.sub.fastq    29854     26895
# 16S-H3_R1.sub.fastq    33134     28669
# 16S-H4_R1.sub.fastq    34295     30067
# 16S-H5_R1.sub.fastq    27049     23639
# 16S-M1_R1.sub.fastq    28653     23677
# 16S-M2_R1.sub.fastq    30647     25269
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences
```{r}
# Learn error rates specific for Miseq sequencing
errF_16S_M <- learnErrors(filtFs_16S_M, multithread=TRUE)
errR_16S_M <- learnErrors(filtRs_16S_M, multithread=TRUE)

# Call ASVs for sequences
dadaFs_16S_M <- dada(filtFs_16S_M, err=errF_16S_M, multithread=TRUE)
dadaRs_16S_M <- dada(filtRs_16S_M, err=errR_16S_M, multithread=TRUE)

#make contigs between the forward and reverse reads. 
mergers_16S_M <- mergePairs(dadaFs_16S_M, filtFs_16S_M, dadaRs_16S_M, filtRs_16S_M, verbose=TRUE, justConcatenate = TRUE)

# Make a sequence table across all ASVs
seqtab_16S_M <- makeSequenceTable(mergers_16S_M)

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_16S_M <- removeBimeraDenovo(seqtab_16S_M, method="consensus", multithread=TRUE, verbose=TRUE)

#Track sequences throughout the analysis to look for large read loss after certain steps
track_16S_M <- cbind(out_16S_M, sapply(dadaFs_16S_M, getN), sapply(dadaRs_16S_M, getN), sapply(mergers_16S_M, getN), rowSums(seqtab.nochim_16S_M))
colnames(track_16S_M) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track_16S_M) <- sample.names_16S_M
track_16S_M

#        input filtered denoisedF denoisedR merged nonchim
# 16S-GM 32445    28978     28816     28816  28723   27252
# 16S-H1 31653    28178     27655     27603  27273   24738
# 16S-H2 29854    26895     26413     26448  26130   24232
# 16S-H3 33134    28669     28135     28174  27916   27018
# 16S-H4 34295    30067     29575     29443  29195   27185
# 16S-H5 27049    23639     23175     23246  22962   21073
# 16S-M1 28653    23677     23240     21695  21471   19827
# 16S-M2 30647    25269     24584     24643  24251   22689
```

##### Assign taxonomy and export files
```{r}
# Assign taxonomy using the silva_nr99_v138.1_train_set.fa.gz file that can be downloaded from https://zenodo.org/record/4587955 (and is mentioned in the dada2 tutorial referenced above)
taxa_16S_M <- assignTaxonomy(seqtab.nochim_16S_M, "silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)

# Write out taxonomy file. 
write.csv(taxa_16S_M , file="Output/16s_gyrB_miseq_tax.csv")

# Write out ASV table
write.csv(seqtab.nochim_16S_M, file="Output/16S_gyrB_miseq_seq.csv")
```

## Analyze the Pseudo-Miseq Reads for the Bacteroidaceae family 

This data is created from using *in silico* PCR on the long read PacBio data with the original Moeller short read primer sets. 


##### Set primer sequences and read in sequence data 

```{r}
#These are the primers created with phylotags, without the PacBio overhangs
F_Bac_cut<- "CGGAGGTAARTTCGAYAAAGG"
R_Bac_cut<-"GCRTATTTYTTCARHGTACGG"


# Input the fasta sequences from the input directory 
fns_bac_cut <- list.files(path_pseudo_Bac, pattern="fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fns_bac_cut
```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size
```{r}
# Remove primers from the PacBio sequences 
nops2_bac_cut <- file.path(path_pseudo_Bac, "noprimers", basename(fns_bac_cut))
for(i in seq_along(fns_bac_cut)){
  fn_bac_cut <- fns_bac_cut[[i]]; nop_bac_cut <- nops2_bac_cut[[i]]
  dada2:::removePrimers(fn_bac_cut, nop_bac_cut, primer.fwd=F_Bac_cut, primer.rev=dada2:::rc(R_Bac_cut), orient=TRUE)
}

# Filter and trim reads with a minimum quality score of 3, minimum length of 550bp, maximum length of 650bp, zero ambiguous bases (Ns), don't remove phiX sequences, and a maximum expected error rate (maxEE) of 3.

filts2_bac_cut <- file.path(path_pseudo_Bac, "noprimers", "filtered", basename(fns_bac_cut))
track2_bac_cut <- filterAndTrim(fns_bac_cut, filts2_bac_cut, minQ=3, minLen=550, maxLen=650, maxN=0, rm.phix = FALSE, maxEE=3)



# print statistics for how many reads were filtered out
track2_bac_cut

#                          reads.in reads.out
# Bac_GM_cut_pac.sub.fastq     6198      5216
# Bac_H1_cut_pac.sub.fastq     8540      7208
# Bac_H2_cut_pac.sub.fastq     6433      5396
# Bac_H3_cut_pac.sub.fastq    10092      8508
# Bac_H4_cut_pac.sub.fastq     6669      6051
# Bac_H5_cut_pac.sub.fastq     2716      2477
# Bac_H6_cut_pac.sub.fastq     2716      2477
# Bac_M1_cut_pac.sub.fastq     7919      6711
# Bac_M2_cut_pac.sub.fastq     7338      6311
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences

```{r}
# Dereplicate filtered files:
drep2_bac_cut <- derepFastq(filts2_bac_cut, verbose=TRUE)

# Learn error rates specific for PacBio sequencing:
err2_bac_cut <- learnErrors(drep2_bac_cut, errorEstimationFunction = PacBioErrfun, BAND_SIZE=32, multithread = TRUE)

# Call ASVs for sequences
dd2_bac_cut <-dada(drep2_bac_cut, err=err2_bac_cut, multithread = TRUE)

# Make a sequence table across all ASVs
st2_bac_cut <- makeSequenceTable(dd2_bac_cut) ; dim(st2_bac_cut)



# Check for the number if chimeric sequences 
bim2_bac_cut <- isBimeraDenovo(st2_bac_cut, minFoldParentOverAbundance = 3.5, multithread = TRUE)
table(bim2_bac_cut)

# FALSE  TRUE 
#    32    21 

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_bac_cut <- removeBimeraDenovo(st2_bac_cut, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_bac_cut)/sum(st2_bac_cut)

# [1] 0.9951291
```

##### Relabel samples and and assign taxonomy
```{r}
# Relabel chimera filtered samples with appropriate sample names. This command is path dependent so it needs to be modified based on where the test directory is 
sample.names_bac_cut <- sapply(strsplit(fns_bac_cut,"/"), function(x) paste(x[8]))
row.names(seqtab.nochim_bac_cut) <- sample.names_bac_cut

# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file:

bac_gryB_tax_cut <- assignTaxonomy(seqtab.nochim_bac_cut, "gyr.refseq.fa.s.fa")


#We will also use the original PacBio sequences to assign the cut reads to their parent PacBio sequences
bac_gryB_tax_cut2 <- assignTaxonomy(seqtab.nochim_bac_cut, "Bac_Hum_species.fasta")
```

##### Write out ASV table and taxonomy assignments

```{r}
# Write out ASV table
write.csv(seqtab.nochim_bac_cut,file="Output/bac_gyrB_pac_seq_cut.csv")

# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast. 
write.csv(bac_gryB_tax_cut, file="Output/bac_gyrB_pac_tax_cut.csv")

#Write out parent to pseudo classification 
write.csv(bac_gryB_tax_cut2, file="Output/bac_gyrB_pac_cut_to_pseudo.csv")
```

## Analyze the Pseudo-Miseq Reads for the Bifidobacteriaceae family 

This data is created from using *in silico* PCR on the long read PacBio data with the original Moeller short read primer sets. 

##### Set primer sequences and read in sequence data 

```{r}
#These are the primers created with phylotags, without the PacBio overhangs
F_Bif_cut <- "GACRACGGNCGNGGCATYCC"
R_Bif_cut <-"AGNCCCTTGTTNAGGAAVGCC"

# Input the fasta sequences from the input directory 
fns_bif_cut <- list.files(path_pseudo_Bif, pattern="fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fns_bif_cut
```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size

```{r}
# Remove primers from the PacBio sequences 
nops2_bif_cut <- file.path(path_pseudo_Bif, "noprimers", basename(fns_bif_cut))
for(i in seq_along(fns_bif_cut)){
  fn_bif_cut <- fns_bif_cut[[i]]; nop_bif_cut <- nops2_bif_cut[[i]]
  dada2:::removePrimers(fn_bif_cut, nop_bif_cut, primer.fwd=F_Bif_cut, primer.rev=dada2:::rc(R_Bif_cut), orient=TRUE)
}

# Filter and trim reads with a minimum quality score of 3, minimum length of 350bp, maximum length of 450bp, zero ambiguous bases (Ns), don't remove phiX sequences, and a maximum expected error rate (maxEE) of 3.

filts2_bif_cut <- file.path(path_pseudo_Bif, "noprimers", "filtered", basename(fns_bif_cut))
track2_bif_cut <- filterAndTrim(nops2_bif_cut, filts2_bif_cut, minQ=3, minLen=350, maxLen=450, maxN=0, rm.phix = FALSE, maxEE=3)

# print statistics for how many reads were filtered out
track2_bif_cut
#                          reads.in reads.out
# Bif_GM_cut_pac.sub.fastq     8928      7821
# Bif_H1_cut_pac.sub.fastq     7259      6342
# Bif_H2_cut_pac.sub.fastq     7420      6460
# Bif_H4_cut_pac.sub.fastq    10131      8794
# Bif_H5_cut_pac.sub.fastq     4562      4009
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences
```{r}
# Dereplicate filtered files:
drep2_bif_cut <- derepFastq(filts2_bif_cut, verbose=TRUE)

# Learn error rates specific for PacBio sequencing:
err2_bif_cut <- learnErrors(drep2_bif_cut, errorEstimationFunction = PacBioErrfun, BAND_SIZE=32, multithread = TRUE)

# Call ASVs for sequences
dd2_bif_cut <- dada(drep2_bif_cut, err=err2_bif_cut, multithread = TRUE)

# Make a sequence table across all ASVs
st2_bif_cut <- makeSequenceTable(dd2_bif_cut) ; dim(st2_bif_cut)



# Check for the number if chimeric sequences 
bim2_bif_cut <- isBimeraDenovo(st2_bif_cut, minFoldParentOverAbundance = 3.5, multithread = TRUE)
table(bim2_bif_cut)

# FALSE  TRUE 
#    12    22 

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_bif_cut <- removeBimeraDenovo(st2_bif_cut, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_bif_cut)/sum(st2_bif_cut)
# [1] 0.9861294
```


##### Relabel samples and and assign taxonomy
```{r}
# Relabel chimera filtered samples with appropriate sample names. This command is path dependent so it needs to be modified based on where the test directory is 
sample.names_bif_cut <- sapply(strsplit(fns_bif_cut,"/"), function(x) paste(x[8]))
sum(seqtab.nochim_bif_cut)/sum(st2_bif_cut)
row.names(seqtab.nochim_bif_cut) <- sample.names_bif_cut

# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file:

bif_gryB_tax_cut <- assignTaxonomy(seqtab.nochim_bif_cut, "gyr.refseq.fa.s.fa")

#We will also use the original PacBio sequences to assign the cut reads to their parent PacBio sequences
bif_gryB_tax_cut2 <- assignTaxonomy(seqtab.nochim_bac_cut, "Bif_pac_spec.fasta")


```

##### Write out ASV table and taxonomy assignments
```{r}
# Write out ASV table
write.csv(seqtab.nochim_bif_cut,file="Output/bif_gyrB_pac_seq_cut.csv")

# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast. 
write.csv(bif_gryB_tax_cut, file="Output/bif_gyrB_pac_tax_cut.csv")

#Write out parent to pseudo classification 
write.csv(bif_gryB_tax_cut2, file="Output/bif_gyrB_pac_cut_to_pseudo.csv")
```

## Analyze the Pseudo-Miseq Reads for the Lachnospiraceae family 

This data is created from using *in silico* PCR on the long read PacBio data with the original Moeller short read primer sets. 

##### Set primer sequences and read in sequence data 

```{r}
#These are the primers created with phylotags, without the PacBio overhangs
F_Lac_cut <- "GGHGGAGGATAYAAGGTATCC"
R_Lac_cut <-"TRTANGAATCRTTRTGCTGC"

# Input the fasta sequences from the input directory 
fns_lac_cut <- list.files(path_pseudo_Lac, pattern="fastq", full.names = TRUE)

# List the fasta sequences to make sure we inputed the right ones 
fns_lac_cut
```

##### Remove primers, filter out low quality reads and filter out reads that are the wrong size
```{r}
# Remove primers from the PacBio sequences 
nops2_lac_cut <-file.path(path_pseudo_Lac, "noprimers", basename(fns_lac_cut))
for(i in seq_along(fns_lac_cut)){
  fn_lac_cut <- fns_lac_cut[[i]]; nop_lac_cut <- nops2_lac_cut[[i]]
  dada2:::removePrimers(fn_lac_cut, nop_lac_cut, primer.fwd=F_Lac_cut, primer.rev=dada2:::rc(R_Lac_cut), orient=TRUE)
}
# Filter and trim reads with a minimum quality score of 3, minimum length of 400bp, maximum length of 500bp, zero ambiguous bases (Ns), don't remove phiX sequences, and a maximum expected error rate (maxEE) of 3.
filts2_lac_cut <- file.path(path_pseudo_Lac, "noprimers", "filtered", basename(fns_lac_cut))
track2_lac_cut <- filterAndTrim(nops2_lac_cut, filts2_lac_cut, minQ=3, minLen=400, maxLen=500, maxN=0, rm.phix = FALSE, maxEE=3)

# print statistics for how many reads were filtered out
track2_lac_cut

#                          reads.in reads.out
# Lac_GM_cut_pac.sub.fastq     5796      5054
# Lac_H1_cut_pac.sub.fastq    16499     14536
# Lac_H2_cut_pac.sub.fastq    13918     12302
# Lac_H3_cut_pac.sub.fastq     9053      7967
# Lac_H4_cut_pac.sub.fastq     4636      3954
# Lac_H5_cut_pac.sub.fastq    20499     17926
# Lac_M1_cut_pac.sub.fastq    14271     10960
# Lac_M2_cut_pac.sub.fastq     8675      7033
```

##### Dereplicate the trimmed fastq files and learn errors, call ASVs, and remove chimeric sequences
```{r}
# Dereplicate filtered files:
drep2_lac_cut <- derepFastq(filts2_lac_cut, verbose=TRUE)

# Learn error rates specific for PacBio sequencing:
err2_lac_cut <- learnErrors(drep2_lac_cut, errorEstimationFunction = PacBioErrfun, BAND_SIZE=32, multithread = TRUE)

# Call ASVs for sequences
dd2_lac_cut <- dada(drep2_lac_cut, err=err2_lac_cut, multithread = TRUE)

# Make a sequence table across all ASVs
st2_lac_cut <- makeSequenceTable(dd2_lac_cut) ; dim(st2_lac_cut)



# Check for the number if chimeric sequences 
bim2_lac_cut <- isBimeraDenovo(st2_lac_cut, minFoldParentOverAbundance = 3.5, multithread = TRUE)
table(bim2_lac_cut)

# FALSE  TRUE 
#   154    23 

# Remove chimeras and check for percentage of reads retained 
seqtab.nochim_lac_cut <- removeBimeraDenovo(st2_lac_cut, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim_lac_cut)/sum(st2_lac_cut)

# [1] 0.9935368
```

##### Relabel samples and and assign taxonomy
```{r}
# Relabel chimera filtered samples with appropriate sample names. This command is path dependent so it needs to be modified based on where the test directory is 
sample.names_lac_cut<- sapply(strsplit(fns_lac_cut,"/"), function(x) paste(x[7]))
row.names(seqtab.nochim_lac_cut) <- sample.names_lac_cut

# Assign taxonomy using the custom gyrB database generated by gyrB_database_creation.sh script and modified by the extra bash script at the end of this rmarkdown file:
lac_gryB_tax_cut <- assignTaxonomy(seqtab.nochim_lac_cut, "gyr.refseq.fa.s.fa")


#We will also use the original PacBio sequences to assign the cut reads to their parent PacBio sequences
lac_gryB_tax_cut_pac_match <- assignTaxonomy(seqtab.nochim_lac_cut, "Updated_Lac_pac_all.s.fa")
```

##### Write out ASV table and taxonomy assignments
```{r}
# Write out ASV table
write.csv(seqtab.nochim_lac_cut,file="Output/lac_gyrB_pac_seq_cut.csv")

# Write out taxonomy file. Note that this classification is not perfect, unclassified sequences were classified via Blast. 
write.csv(lac_gryB_tax_cut, file="Output/lac_gyrB_pac_tax_cut.csv")

#Write out parent to pseudo classification 
write.csv(lac_gryB_tax_cut_pac_match, file="Output/lac_gyrB_pac_cut_to_pseudo.csv")
```


# Extra scripts 

##### Convert the gyrB reference database into a usable database for dada2
```{bash}
#move into the test directory 
mv ./GyrB_Test_Data
#add a semicolon to the end of each line 
sed 's/$/;/;n' gyr_refseq.fa.txt > gyr.refseq.fa.s.txt
```
